2026-01-12T17:37:13.640185Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T17:37:13.640884Z  INFO bit_llama::train::training_loop: --- Bit-Llama Training (Pure Rust) ---
2026-01-12T17:37:13.640895Z  INFO bit_llama::train::training_loop: Config: Dim=64, Layers=2, Context=16, Batch=2
2026-01-12T17:37:13.640904Z  INFO bit_llama::train::training_loop: Hyperparams: LR=0.0003, Steps=10, Warmup=100, MinLR=0.00001
2026-01-12T17:37:13.640907Z  INFO bit_llama::train::training_loop: üìä Model Size: 0.10M Params
2026-01-12T17:37:13.640921Z  INFO bit_llama::train::training_loop: üíæ Est. VRAM Usage: 513.52 MB (Params: 1.50 + Act: 0.02)
2026-01-12T17:37:13.640924Z  INFO bit_llama::train::training_loop: Initializing Device...
2026-01-12T17:37:13.640930Z  INFO bit_llama::train::training_loop: Device initialized: Cpu
2026-01-12T17:37:13.641167Z  INFO bit_llama::train::training_loop: Loading Tokenizer from: "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT\\data\\smoke_test\\tokenizer.json"
2026-01-12T17:37:13.641203Z  WARN bit_llama::train::training_loop: ‚ö†Ô∏è Tokenizer not found! Specific path: "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT\\data\\smoke_test\\tokenizer.json"
2026-01-12T17:37:13.641207Z  WARN bit_llama::train::training_loop: ‚ö†Ô∏è Defaulting VOCAB to 16384 (Risk of mismatch!)
2026-01-12T17:37:13.643836Z  INFO bit_llama::loader: BitLoader: Loading "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT\\data\\smoke_test\\dummy.u32" (Model: u32, Tokens: 1000)
2026-01-12T17:37:13.643849Z  INFO bit_llama::train::training_loop: Data Loaded. Total tokens: 1000
2026-01-12T17:37:13.668623Z  INFO bit_llama::train::training_loop: No checkpoint found. Starting fresh.
2026-01-12T17:37:13.668643Z  INFO bit_llama::train::training_loop: Model initialized. Varmap Key count: 17
2026-01-12T17:37:13.669166Z  INFO bit_llama::train::training_loop: Starting Training Loop (Target: 10 steps, Save every 500 steps)...
2026-01-12T17:37:13.669249Z  INFO bit_llama::train::training_loop: CWD: "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT"
2026-01-12T17:37:13.669689Z  INFO bit_llama::train::training_loop: üìÅ Output directory: pre_demon_test
2026-01-12T17:37:13.670190Z  INFO bit_llama::train::training_loop: ‚úÖ Project Config saved to: pre_demon_test/project.json
2026-01-12T17:37:13.670561Z  INFO bit_llama::train::training_loop: ‚úÖ Model Config saved to: pre_demon_test/config.json
2026-01-12T17:37:13.678318Z  INFO bit_llama::train::training_loop: ‚úÖ Tokenizer backed up to: pre_demon_test/tokenizer.json (from default path)
2026-01-12T17:37:13.845843Z  INFO bit_llama::train::training_loop: Step    0 | Loss: 10.6677 | LR: 0.0000000 | 191.05 tok/s
2026-01-12T17:37:15.199884Z  INFO bit_llama::train::training_loop: Training complete. Saving final model...
2026-01-12T17:37:15.206769Z  INFO bit_llama::train::training_loop: ‚úÖ Model saved to: pre_demon_test/model.safetensors
2026-01-12T17:37:15.207249Z  INFO bit_llama::train::training_loop: ‚úÖ Project Config saved to: pre_demon_test/project.json
2026-01-12T17:37:15.207573Z  INFO bit_llama::train::training_loop: ‚úÖ Model Config saved to: pre_demon_test/config.json
2026-01-12T19:12:02.747889Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T19:49:04.751053Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T19:49:04.751432Z  INFO bit_llama::train::training_loop: --- Bit-Llama Training (Pure Rust) ---
2026-01-12T19:49:04.751436Z  INFO bit_llama::train::training_loop: Config: Dim=64, Layers=2, Context=16, Batch=2
2026-01-12T19:49:04.751440Z  INFO bit_llama::train::training_loop: Hyperparams: LR=0.0003, Steps=10, Warmup=100, MinLR=0.00001
2026-01-12T19:49:04.751446Z  INFO bit_llama::train::training_loop: üìä Model Size: 0.10M Params
2026-01-12T19:49:04.751454Z  INFO bit_llama::train::training_loop: üíæ Est. VRAM Usage: 513.52 MB (Params: 1.50 + Act: 0.02)
2026-01-12T19:49:04.751456Z  INFO bit_llama::train::training_loop: Initializing Device...
2026-01-12T19:49:04.751461Z  INFO bit_llama::train::training_loop: Device initialized: Cpu
2026-01-12T19:49:04.751547Z  INFO bit_llama::train::training_loop: Loading Tokenizer from: "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT\\data\\smoke_test\\tokenizer.json"
2026-01-12T19:49:04.751566Z  WARN bit_llama::train::training_loop: ‚ö†Ô∏è Tokenizer not found! Specific path: "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT\\data\\smoke_test\\tokenizer.json"
2026-01-12T19:49:04.751570Z  WARN bit_llama::train::training_loop: ‚ö†Ô∏è Defaulting VOCAB to 16384 (Risk of mismatch!)
2026-01-12T19:49:04.752278Z  INFO bit_llama::loader: BitLoader: Loading "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT\\data\\smoke_test\\dummy.u32" (Model: u32, Tokens: 1000)
2026-01-12T19:49:04.752284Z  INFO bit_llama::train::training_loop: Data Loaded. Total tokens: 1000
2026-01-12T19:49:04.765671Z  INFO bit_llama::train::training_loop: No checkpoint found. Starting fresh.
2026-01-12T19:49:04.765681Z  INFO bit_llama::train::training_loop: Model initialized. Varmap Key count: 17
2026-01-12T19:49:04.765945Z  INFO bit_llama::train::training_loop: Starting Training Loop (Target: 10 steps, Save every 500 steps)...
2026-01-12T19:49:04.765983Z  INFO bit_llama::train::training_loop: CWD: "C:\\Users\\Humin\\.gemini\\antigravity\\scratch\\new-ai-project\\Bit-TTT"
2026-01-12T19:49:04.766107Z  INFO bit_llama::train::training_loop: üìÅ Output directory: pre_demon_test
2026-01-12T19:49:04.766358Z  INFO bit_llama::train::training_loop: ‚úÖ Project Config saved to: pre_demon_test/project.json
2026-01-12T19:49:04.766731Z  INFO bit_llama::train::training_loop: ‚úÖ Model Config saved to: pre_demon_test/config.json
2026-01-12T19:49:04.770550Z  INFO bit_llama::train::training_loop: ‚úÖ Tokenizer backed up to: pre_demon_test/tokenizer.json (from default path)
2026-01-12T19:49:04.834903Z  INFO bit_llama::train::training_loop: Step    0 | Loss: 10.0929 | LR: 0.0000000 | 497.51 tok/s
2026-01-12T19:49:05.366900Z  INFO bit_llama::train::training_loop: Training complete. Saving final model...
2026-01-12T19:49:05.371295Z  INFO bit_llama::train::training_loop: ‚úÖ Model saved to: pre_demon_test/model.safetensors
2026-01-12T19:49:05.371499Z  INFO bit_llama::train::training_loop: ‚úÖ Project Config saved to: pre_demon_test/project.json
2026-01-12T19:49:05.371676Z  INFO bit_llama::train::training_loop: ‚úÖ Model Config saved to: pre_demon_test/config.json
2026-01-12T19:58:04.768988Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T19:58:05.035340Z  INFO bit_llama::gui: [GUI] Loading bundled font: NotoSansJP-Regular.ttf
2026-01-12T19:58:05.035630Z  INFO bit_llama::gui: [GUI] Fonts initialized.
2026-01-12T19:59:19.273083Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T19:59:19.494979Z  INFO bit_llama::gui: [GUI] Loading bundled font: NotoSansJP-Regular.ttf
2026-01-12T19:59:19.495029Z  INFO bit_llama::gui: [GUI] Fonts initialized.
2026-01-12T20:44:18.577773Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T20:44:18.800220Z  INFO bit_llama::gui: [GUI] Loading bundled font: NotoSansJP-Regular.ttf
2026-01-12T20:44:18.800269Z  INFO bit_llama::gui: [GUI] Fonts initialized.
2026-01-12T20:50:27.587345Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T20:50:27.832522Z  INFO bit_llama::gui: [GUI] Loading bundled font: NotoSansJP-Regular.ttf
2026-01-12T20:50:27.832566Z  INFO bit_llama::gui: [GUI] Fonts initialized.
2026-01-12T20:55:04.570079Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T20:55:04.708559Z  INFO bit_llama::gui: [GUI] Loading bundled font: NotoSansJP-Regular.ttf
2026-01-12T20:55:04.708608Z  INFO bit_llama::gui: [GUI] Fonts initialized.
2026-01-12T20:57:12.621975Z  INFO bit_llama: üöÄ Bit-Llama started.
2026-01-12T20:57:12.844323Z  INFO bit_llama::gui: [GUI] Loading bundled font: NotoSansJP-Regular.ttf
2026-01-12T20:57:12.844377Z  INFO bit_llama::gui: [GUI] Fonts initialized.
