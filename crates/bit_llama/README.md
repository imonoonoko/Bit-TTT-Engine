# Bit-Llama Construction Report (Phase 13)

## ðŸ“Œ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
(Work In Progress)


---

## ðŸ“‚ æˆæžœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### 1. **ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ (è„³ã®æ§‹é€ )**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/core_engine.rs`
*   **é€²åŒ–ç‚¹**:
    *   `BitLlama` æ§‹é€ ä½“ã®å®Ÿè£…ï¼ˆEmbedding -> Nå±¤ -> Headï¼‰ã€‚
    *   `TTTLayer` ã®ãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼ˆ`B, T, D`ï¼‰ã€‚
    *   `RMSNorm` ã¨ `SwiGLU` (MLP) ã®å®Ÿè£…ã«ã‚ˆã‚‹è¡¨ç¾åŠ›å‘ä¸Šã€‚
    *   æ®‹å·®æŽ¥ç¶šï¼ˆResidual Connectionsï¼‰ã®å°Žå…¥ã€‚
    *   **æ³¨æ„**: æœ¬ã‚¯ãƒ¬ãƒ¼ãƒˆã¯ `../rust_engine` ã®ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å‚ç…§ã—ã¦ã„ã¾ã™ã€‚ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã‚’å¤‰æ›´ã›ãšã€ `Bit-TTT` ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ãŠä½¿ã„ãã ã•ã„ã€‚

### 2. **ãƒ‡ãƒ¼ã‚¿æº–å‚™ (æ•™æ)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `data_prep/prepare_tinystories.py`
*   **æ©Ÿèƒ½**:
    *   TinyStoriesãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚
    *   å°‚ç”¨BPEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®å­¦ç¿’ï¼ˆèªžå½™æ•° 16,384ï¼‰ã€‚
    *   Rustã§ã®é«˜é€Ÿèª­ã¿è¾¼ã¿ç”¨ã« `u16` ãƒã‚¤ãƒŠãƒªå½¢å¼ (`train.bin`) ã¸å¤‰æ›ã€‚

### 3. **å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (è„³ã®è‚²æˆ)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/bin/train_llama.rs`
*   **æ©Ÿèƒ½**:
    *   **GPUãƒãƒƒãƒå­¦ç¿’**: `BATCH_SIZE=32` ã§8GB VRAMã«æœ€é©åŒ–ã€‚
    *   **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½**: 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è‡ªå‹•ä¿å­˜ã—ã€ä¸­æ–­ãƒ»å†é–‹ãŒå¯èƒ½ã€‚
    *   **é«˜é€ŸåŒ–**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’128ã«èª¿æ•´ã—ã€å›žè»¢çŽ‡ã‚’å‘ä¸Šã€‚
*   **ã‚³ãƒžãƒ³ãƒ‰**:
    ```cmd
    cargo run --release --features cuda --bin train_llama
    ```

### 4. **æŽ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãŠã—ã‚ƒã¹ã‚Š)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/bin/inference_llama.rs`
*   **æ©Ÿèƒ½**: å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿ã€å¯¾è©±å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚

---

## ðŸ› ï¸ Development Guide (Experimental)

The following steps are for **active development and debugging only**.

### 1. Requirements
*   Python 3.x
*   Rust Toolchain (Cargo)
*   CUDA Toolkit (Optional, for GPU training)

### 2. Run Training (Dev Mode)
```bash
# GPU Mode (Requires NVIDIA GPU)
cargo run --release --features cuda --bin train_llama

# CPU Mode
cargo run --release --bin train_llama
```

> **Warning**: This is a prototype implementation. Parameters and data formats may change.

