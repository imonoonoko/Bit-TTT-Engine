use candle_core::{DType, Device, Result};
use candle_nn::VarBuilder;

fn main() -> Result<()> {
    // ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
    let model_path = "models/TinyLlama-Adaptive-1.1B/model.safetensors";
    let device = Device::Cpu;
    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[model_path], DType::F32, &device)? };

    println!("ğŸ¥ Checking IO Layers...");

    // 1. LM Head Check (æœ€é‡è¦å®¹ç–‘è€…)
    let head = vb.get((32000, 2048), "lm_head.weight")?;
    let head_vec = head.flatten_all()?.to_vec1::<f32>()?;

    println!("\nğŸ’€ LM Head Stats:");
    println!("   Shape: {:?}", head.shape());
    println!("   First 5: {:?}", &head_vec[..5]); // ã“ã‚ŒãŒ Pythonã®å…ƒãƒ¢ãƒ‡ãƒ«ã¨ä¸€è‡´ã™ã‚‹ã‹ï¼Ÿ
    println!("   Last 5:  {:?}", &head_vec[head_vec.len() - 5..]);

    let head_std = (head.sqr()?.sum_all()?.to_scalar::<f32>()? / head_vec.len() as f32).sqrt();
    println!("   Std Dev: {:.6} (Should be ~0.13)", head_std); // é‡ã¿ã®åˆ†æ•£ãƒã‚§ãƒƒã‚¯

    // 2. Final Norm Check (å…±çŠ¯ã®å¯èƒ½æ€§)
    // TinyLlamaã¯ RMSNorm ãªã®ã§ weight ã®ã¿ (biasãªã—)
    let norm = vb.get((2048,), "model.norm.weight")?;
    let norm_vec = norm.flatten_all()?.to_vec1::<f32>()?;

    println!("\nğŸ›¡ï¸ Final Norm Stats:");
    println!("   Shape: {:?}", norm.shape());
    println!("   First 5: {:?}", &norm_vec[..5]); // é€šå¸¸ã¯ 1.0 ã«è¿‘ã„å€¤ãªã©
    println!(
        "   Mean:    {:.6}",
        norm_vec.iter().sum::<f32>() / norm_vec.len() as f32
    );

    // 3. Token Embed Check (å…¥å£)
    let embed = vb.get((32000, 2048), "model.embed_tokens.weight")?;
    let embed_vec = embed.flatten_all()?.to_vec1::<f32>()?;
    println!("\nğŸšª Embed Token Stats:");
    println!("   First 5: {:?}", &embed_vec[..5]);

    Ok(())
}
