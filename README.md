# Bit-TTT Engine: High-Performance Brain Core
**1.58-bit Quantization + Test-Time Training (TTT)** Implementation in Rust.

[Japanese / æ—¥æœ¬èª](#japanese) below.

---

<a name="english"></a>
# ğŸ‡¬ğŸ‡§ English: Bit-TTT Engine

## Overview
**Bit-TTT Engine** is a high-performance implementation of the Bit-TTT architecture. It combines **1.58-bit quantization efficiency** with **Test-Time Training (TTT)** adaptability. It runs entirely on the CPU using optimized integer arithmetic and SIMD/AVX instructions, achieving extreme throughput (**30,000+ TPS**).

ğŸ“˜ **[Read the Architecture Design](ARCHITECTURE.md)** to understand the core philosophy.



## Features
*   **Ultra Fast**: Optimizes matrix operations using `i8` integers and AVX2/AVX-512 instructions.
*   **Adaptive Memory**: Updates its internal state in real-time for every input token (online learning).
*   **Portable**: Distributed as a standard generic DLL/Shared Library (`release/Bit_TTT.dll`), usable from Python, C#, Unity, C++, etc.
*   **Safe**: Safe C-ABI with error codes and documented safety contracts.

## Project Components
- **[`bit_llama/`](bit_llama/)**: (New!) Pure Rust implementation of "Bit-Llama" (Stacked Bit-TTT). Supports GPU training and TinyStories generation.
- **[`rust_engine/`](rust_engine/)**: Core logic optimized for C-ABI (DLL generation).
- **[`examples/`](examples/)**: Minimal usage examples (Python etc).
- **[`python_proto/`](python_proto/)**: Original Python prototype for research.

## Quick Start (Python)

> **Want to train an LLM?**  
> Go to **[`bit_llama/README.md`](bit_llama/README.md)** for instructions on training "Bit-Llama" on TinyStories.

To try the Core Engine directly via Python C-API:

```bash
# Verify behavior and speed
python examples/python_inference.py
```

Expected Output:
```text
Running Inference on 10 tokens...
Done in 0.0003 sec.
Output Shape: 640 floats
Success! w_state has been updated internally.
```

(For detailed benchmarking, run `python release/benchmark.py`)

## Developer Guide (C-ABI)
For integration with C, C++, or C# (Unity), use the exported functions:

### Error Codes
| Code | Name | Description |
|---|---|---|
| **0** | `Ok` | Success |
| **1** | `NullPointer` | Input pointer was null |
| **2** | `DimensionMismatch` | Input array size validation failed |
| **99** | `Panic` | Internal Rust panic caught |

### API Signature
```c
// Create Model: returns ptr or NULL
void* ttt_create(size_t hidden_dim, float inner_lr);

// Forward + Update: returns error code (0 = Ok)
int ttt_forward(void* model, const float* input, size_t seq_len, float* output);

// Destroy Model
void ttt_destroy(void* model);
```

---

<a name="japanese"></a>
# ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª: Bit-TTT è„³ã‚¨ãƒ³ã‚¸ãƒ³

## æ¦‚è¦
**Bit-TTT Engine** ã¯ã€Bit-TTTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é«˜æ€§èƒ½å®Ÿè£…ç‰ˆã§ã™ã€‚**1.58bité‡å­åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡æ€§**ã¨ã€**Test-Time Training (æ¨è«–æ™‚å­¦ç¿’) ã«ã‚ˆã‚‹é©å¿œæ€§**ã‚’å…¼ã­å‚™ãˆã¦ã„ã¾ã™ã€‚
å®Œå…¨ã«CPUä¸Šã§å‹•ä½œã—ã€SIMD/AVXå‘½ä»¤ã‚’é§†ä½¿ã—ãŸæ•´æ•°æ¼”ç®—ã«ã‚ˆã‚Šã€ä¸€èˆ¬çš„ãªPCã§ **30,000+ TPS (ãƒˆãƒ¼ã‚¯ãƒ³/ç§’)** ã¨ã„ã†é©šç•°çš„ãªæ¨è«–é€Ÿåº¦ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

ğŸ“˜ **[ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆæ›¸ (æ—¥æœ¬èª)](ARCHITECTURE_JA.md)** ã‚‚å‚ç…§ã—ã¦ãã ã•ã„ã€‚



## ç‰¹å¾´
*   **çˆ†é€Ÿ**: `i8` æ•´æ•°æ¼”ç®—ã¨AVX2/AVX-512å‘½ä»¤ã‚»ãƒƒãƒˆã«ã‚ˆã‚Šæœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚
*   **å­¦ç¿’ã™ã‚‹è¨˜æ†¶**: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å—ã‘å–ã‚‹ãŸã³ã«ã€å†…éƒ¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›´æ–°ï¼ˆå­¦ç¿’ï¼‰ã—ã¾ã™ã€‚
*   **ãƒãƒ¼ã‚¿ãƒ–ãƒ«**: æ±ç”¨çš„ãª DLL (`release/Bit_TTT.dll`) ã¨ã—ã¦æä¾›ã•ã‚Œã‚‹ãŸã‚ã€Python, Unity (C#), C++, Node.js ãªã©ã‚ã‚‰ã‚†ã‚‹ç’°å¢ƒã‹ã‚‰åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
*   **å®‰å…¨**: ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ä¾‹å¤–åˆ¶å¾¡ã¨ã€æ˜ç¢ºãªå®‰å…¨æ€§ä¿è¨¼ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ
- **[`bit_llama/`](bit_llama/)**: (New!) "Bit-Llama" (å¤šå±¤åŒ–Bit-TTT) ã®Pure Rustå®Ÿè£…ã€‚GPUå­¦ç¿’ã¨TinyStoriesç”Ÿæˆã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
- **[`rust_engine/`](rust_engine/)**: C-ABI (DLLç”Ÿæˆ) ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ã§ã™ã€‚
- **[`examples/`](examples/)**: Pythonç­‰ã‹ã‚‰ã®æœ€å°åˆ©ç”¨ä¾‹ã§ã™ã€‚
- **[`python_proto/`](python_proto/)**: ç ”ç©¶ç”¨ã®åˆæœŸPythonãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã§ã™ã€‚

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ (Python)

> **LLMã‚’å­¦ç¿’ã•ã›ãŸã„å ´åˆ**  
> **[`bit_llama/README.md`](bit_llama/README.md)** ã‚’ã”è¦§ãã ã•ã„ã€‚ã€ŒBit-Llamaã€ã®å­¦ç¿’æ‰‹é †ï¼ˆTinyStoriesä½¿ç”¨ï¼‰ã‚’è©³è¿°ã—ã¦ã„ã¾ã™ã€‚

Core Engine (C-API) ã®å‹•ä½œã‚’è©¦ã™ã«ã¯ï¼š

```bash
python examples/python_inference.py
```

å®Ÿè¡Œçµæœã®ä¾‹:
```text
Running Inference on 10 tokens...
Done in 0.0003 sec.
Output Shape: 640 floats
Success! w_state has been updated internally.
```

(è©³ç´°ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®šã¯ `python release/benchmark.py` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)

## é–‹ç™ºè€…ã‚¬ã‚¤ãƒ‰ (C-ABI)
Cè¨€èªã€C++ã€C# (Unity) ãªã©ã‹ã‚‰åˆ©ç”¨ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

### ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰
| Code | Name | Description |
|---|---|---|
| **0** | `Ok` | æˆåŠŸ |
| **1** | `NullPointer` | ãƒã‚¤ãƒ³ã‚¿ãŒ null |
| **2** | `DimensionMismatch` | é…åˆ—ã‚µã‚¤ã‚ºä¸æ­£ |
| **99** | `Panic` | å†…éƒ¨ãƒ‘ãƒ‹ãƒƒã‚¯ç™ºç”Ÿ |

### API ã‚·ã‚°ãƒãƒãƒ£
```c
// ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆ
void* ttt_create(size_t hidden_dim, float inner_lr);

// æ¨è«–å®Ÿè¡Œ (æˆ»ã‚Šå€¤ 0 = æˆåŠŸ)
int ttt_forward(void* model, const float* input, size_t seq_len, float* output);

// ãƒ¢ãƒ‡ãƒ«ç ´æ£„
void ttt_destroy(void* model);
```

---
*Created by Project Bit-TTT.*

