The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.
The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.
Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".
During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.
Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.
In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, for example in language modeling, parsing, and many others.
Large language models (LLMs) are a type of artificial intelligence (AI) algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate, and predict new content. The term generative AI is also closely connected with LLMs, which are, in fact, a type of generative AI that has been specifically architected to help generate text-based content.
The first large language models were developed in the late 2010s. BERT (Bidirectional Encoder Representations from Transformers) was released by Google in 2018. It was a pre-trained model that could be fine-tuned for specific tasks. GPT-2 (Generative Pre-trained Transformer 2) was released by OpenAI in 2019. It was a generative model that could produce coherent text.
GPT-3 was released by OpenAI in 2020. It was a significant improvement over GPT-2, with 175 billion parameters. It demonstrated the ability to perform a wide range of natural language tasks with few-shot learning.
In 2022, OpenAI released ChatGPT, a chatbot based on GPT-3.5. It became very popular and sparked a new wave of interest in LLMs.
Llama (Large Language Model Meta AI) is a family of large language models (LLMs), released by Meta AI starting in February 2023.
For typical PPL evaluation we need a few thousand tokens. This text is about 500 words, ~700 tokens. I'll duplicate it a few times.
The history of natural language processing (NLP) generally started in the 1950s... (Repeat x2)
