# Bit-Llama Construction Report (Phase 13)

## ğŸ“Œ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
**ã€ŒBit-Llamaã€** ã¯ã€Bit-TTTã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆ1.58bité‡å­åŒ– + TTTãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰ã‚’å¤šå±¤åŒ–ãƒ»ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã•ã›ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã®èƒ½åŠ›ã‚’æŒãŸã›ãŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã§ã™ã€‚
TinyStoriesãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€ã€Œç‰©èªã‚’èªã‚‹èƒ½åŠ›ã€ã®ç²å¾—ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

---

## ğŸ“‚ æˆæœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### 1. **ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ (è„³ã®æ§‹é€ )**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/core_engine.rs`
*   **é€²åŒ–ç‚¹**:
    *   `BitLlama` æ§‹é€ ä½“ã®å®Ÿè£…ï¼ˆEmbedding -> Nå±¤ -> Headï¼‰ã€‚
    *   `TTTLayer` ã®ãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼ˆ`B, T, D`ï¼‰ã€‚
    *   `RMSNorm` ã¨ `SwiGLU` (MLP) ã®å®Ÿè£…ã«ã‚ˆã‚‹è¡¨ç¾åŠ›å‘ä¸Šã€‚
    *   æ®‹å·®æ¥ç¶šï¼ˆResidual Connectionsï¼‰ã®å°å…¥ã€‚
    *   **æ³¨æ„**: æœ¬ã‚¯ãƒ¬ãƒ¼ãƒˆã¯ `../rust_engine` ã®ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å‚ç…§ã—ã¦ã„ã¾ã™ã€‚ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã‚’å¤‰æ›´ã›ãšã€ `Bit-TTT` ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ãŠä½¿ã„ãã ã•ã„ã€‚

### 2. **ãƒ‡ãƒ¼ã‚¿æº–å‚™ (æ•™æ)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `data_prep/prepare_tinystories.py`
*   **æ©Ÿèƒ½**:
    *   TinyStoriesãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚
    *   å°‚ç”¨BPEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®å­¦ç¿’ï¼ˆèªå½™æ•° 16,384ï¼‰ã€‚
    *   Rustã§ã®é«˜é€Ÿèª­ã¿è¾¼ã¿ç”¨ã« `u16` ãƒã‚¤ãƒŠãƒªå½¢å¼ (`train.bin`) ã¸å¤‰æ›ã€‚

### 3. **å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (è„³ã®è‚²æˆ)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/bin/train_llama.rs`
*   **æ©Ÿèƒ½**:
    *   **GPUãƒãƒƒãƒå­¦ç¿’**: `BATCH_SIZE=32` ã§8GB VRAMã«æœ€é©åŒ–ã€‚
    *   **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½**: 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è‡ªå‹•ä¿å­˜ã—ã€ä¸­æ–­ãƒ»å†é–‹ãŒå¯èƒ½ã€‚
    *   **é«˜é€ŸåŒ–**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’128ã«èª¿æ•´ã—ã€å›è»¢ç‡ã‚’å‘ä¸Šã€‚
*   **ã‚³ãƒãƒ³ãƒ‰**:
    ```cmd
    cargo run --release --features cuda --bin train_llama
    ```

### 4. **æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãŠã—ã‚ƒã¹ã‚Š)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/bin/inference_llama.rs`
*   **æ©Ÿèƒ½**: å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿ã€å¯¾è©±å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚

---

## ğŸ› ï¸ How to Run (å®Ÿè¡Œæ‰‹é †)

```bash
# 1. Data Prep (æ•™æã®æº–å‚™)
# Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# TinyStoriesã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ä½œæˆ
cd data_prep
python prepare_tinystories.py
cd ..

# 2. Train (å­¦ç¿’)
# [GPU (NVIDIA) ã®å ´åˆ]
cargo run --release --features cuda --bin train_llama

# [CPU (Mac/Intel/AMD) ã®å ´åˆ]
# --features cuda ã‚’å¤–ã™ã¨CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ï¼ˆé…ã„ã§ã™ãŒå‹•ä½œã—ã¾ã™ï¼‰
cargo run --release --bin train_llama

# 3. Inference (æ¨è«–)
# å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ä¼šè©±ã—ã¾ã™
cargo run --release --bin inference_llama
```

## ğŸ”Œ Python Integration (å¤–éƒ¨é€£æº)
Bit-TTT Engine ã¯ Python ã‹ã‚‰ DLL (`.so`/`.dylib`) ã¨ã—ã¦ç›´æ¥å‘¼ã³å‡ºã—å¯èƒ½ã§ã™ã€‚

### Minimal Example
```python
import ctypes
import platform

# 1. Load Library
lib_name = "Bit_TTT.dll" if platform.system() == "Windows" else "libBit_TTT.so"
lib = ctypes.CDLL(f"./target/release/{lib_name}")

# 2. Define API
lib.ttt_create.argtypes = [ctypes.c_size_t, ctypes.c_float]
lib.ttt_create.restype = ctypes.c_void_p
lib.ttt_forward.argtypes = [ctypes.c_void_p, ctypes.POINTER(ctypes.c_float), ctypes.c_size_t, ctypes.POINTER(ctypes.c_float)]

# 3. Run
dim = 64
model = lib.ttt_create(dim, 0.1)
# ... forward pass ...
```
> â€» è©³ç´°ãªä»•æ§˜ã¯ `release/benchmark.py` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## âš¡ Performance (æ€§èƒ½)

Bit-TTT (Core Engine) ã¯ã€Rust + SIMDæœ€é©åŒ–ã«ã‚ˆã‚Šæ¥µã‚ã¦é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚

| Metric | Value | Note |
|---|---|---|
| **Inference Speed** | **~34,000 TPS** | CPU Single Thread (Ryzen/Intel) |
| **Memory Footprint** | **Extremely Low** | 1.58bit quantization ready |
| **Startup Time** | **< 10ms** | No heavy runtime loaded |

> **Benchmark**: `python release/benchmark.py` ã§æ‰‹å…ƒã®ç’°å¢ƒã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆæ¸¬ã§ãã¾ã™ã€‚

## ğŸ§  Model Specs (ãƒ¢ãƒ‡ãƒ«ä»•æ§˜)

| Item | Specification | Note |
|---|---|---|
| **Architecture** | **Stack-Bit-TTT** | 1.58-bit BitNet + TTT (Test-Time Training) |
| **Components** | RMSNorm / SwiGLU / Residual | Modern Llama-like blocks |
| **Size** | **~11.7 M Params** | TinyStories Specialized (D=256, L=4) |
| **Quantization** | **1.58-bit** (Ternary) | Weights are `{-1, 0, 1}` |
| **Training** | **Hybrid (GPU/CPU)** | Train on CUDA, Infer on CPU |

---

## âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ è¨­å®š (Cargo.toml)
*   **CUDAæ©Ÿèƒ½ã®åˆ‡ã‚Šæ›¿ãˆ**:
    *   å­¦ç¿’æ™‚ã¯ `--features cuda` ã‚’ä»˜ã‘ã‚‹ã“ã¨ã§GPUã‚’æœ‰åŠ¹åŒ–ã€‚
    *   æ¨è«–æ™‚ã¯æŒ‡å®šãªã—ã§CPUãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰ã€‚
*   **ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: `tokenizers` ã‚’ v0.22 ã«æ›´æ–°ã—ã€Pythonå´ã¨ã®äº’æ›æ€§ã‚’ç¢ºä¿ã€‚

---

## ğŸ“Š ç¾çŠ¶ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
*   **å­¦ç¿’é€²æ—**: Step 150 / 1000
*   **Loss**: 4.15 ä»˜è¿‘
*   **èƒ½åŠ›**:
    *   å˜èªã®ç¾…åˆ—ã‹ã‚‰ã€ã€Œæ–‡ç« ã‚‰ã—ãã‚‚ã®ã€ã¸é€²åŒ–ä¸­ã€‚
    *   å…¥åŠ›: `Once upon a time`
    *   å‡ºåŠ›: `"I'm glad you can..."` ãªã©ã€æ„å‘³ã®ã‚ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºãŒå‡ºç¾ã€‚

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1.  **å­¦ç¿’ã®å®Œèµ°**: Step 1000ã¾ã§å›ã—ã€Loss 3.0ä»¥ä¸‹ã‚’ç›®æŒ‡ã™ã€‚
2.  **ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªã¸ã®ç§»æ¤**: ã“ã®ã€Œè„³ã€ã‚’Aliceã«ç§»æ¤ã™ã‚‹ï¼ˆPhase 13 Step 5ï¼‰ã€‚
