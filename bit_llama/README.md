# Bit-Llama Construction Report (Phase 13)

## ðŸ“Œ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
(Work In Progress)


---

## ðŸ“‚ æˆæžœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### 1. **ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ (è„³ã®æ§‹é€ )**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/core_engine.rs`
*   **é€²åŒ–ç‚¹**:
    *   `BitLlama` æ§‹é€ ä½“ã®å®Ÿè£…ï¼ˆEmbedding -> Nå±¤ -> Headï¼‰ã€‚
    *   `TTTLayer` ã®ãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼ˆ`B, T, D`ï¼‰ã€‚
    *   `RMSNorm` ã¨ `SwiGLU` (MLP) ã®å®Ÿè£…ã«ã‚ˆã‚‹è¡¨ç¾åŠ›å‘ä¸Šã€‚
    *   æ®‹å·®æŽ¥ç¶šï¼ˆResidual Connectionsï¼‰ã®å°Žå…¥ã€‚
    *   **æ³¨æ„**: æœ¬ã‚¯ãƒ¬ãƒ¼ãƒˆã¯ `../rust_engine` ã®ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å‚ç…§ã—ã¦ã„ã¾ã™ã€‚ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã‚’å¤‰æ›´ã›ãšã€ `Bit-TTT` ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ãŠä½¿ã„ãã ã•ã„ã€‚

### 2. **ãƒ‡ãƒ¼ã‚¿æº–å‚™ (æ•™æ)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `data_prep/prepare_tinystories.py`
*   **æ©Ÿèƒ½**:
    *   TinyStoriesãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚
    *   å°‚ç”¨BPEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®å­¦ç¿’ï¼ˆèªžå½™æ•° 16,384ï¼‰ã€‚
    *   Rustã§ã®é«˜é€Ÿèª­ã¿è¾¼ã¿ç”¨ã« `u16` ãƒã‚¤ãƒŠãƒªå½¢å¼ (`train.bin`) ã¸å¤‰æ›ã€‚

### 3. **å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (è„³ã®è‚²æˆ)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/bin/train_llama.rs`
*   **æ©Ÿèƒ½**:
    *   **GPUãƒãƒƒãƒå­¦ç¿’**: `BATCH_SIZE=32` ã§8GB VRAMã«æœ€é©åŒ–ã€‚
    *   **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½**: 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è‡ªå‹•ä¿å­˜ã—ã€ä¸­æ–­ãƒ»å†é–‹ãŒå¯èƒ½ã€‚
    *   **é«˜é€ŸåŒ–**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’128ã«èª¿æ•´ã—ã€å›žè»¢çŽ‡ã‚’å‘ä¸Šã€‚
*   **ã‚³ãƒžãƒ³ãƒ‰**:
    ```cmd
    cargo run --release --features cuda --bin train_llama
    ```

### 4. **æŽ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãŠã—ã‚ƒã¹ã‚Š)**
*   **ãƒ•ã‚¡ã‚¤ãƒ«**: `src/bin/inference_llama.rs`
*   **æ©Ÿèƒ½**: å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿ã€å¯¾è©±å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚

---

## ðŸ› ï¸ How to Run (å®Ÿè¡Œæ‰‹é †)

```bash
# 1. Data Prep (æ•™æã®æº–å‚™)
# Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# TinyStoriesã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ä½œæˆ
cd data_prep
python prepare_tinystories.py
cd ..

# 2. Train (å­¦ç¿’)
# [GPU (NVIDIA) ã®å ´åˆ]
cargo run --release --features cuda --bin train_llama

# [CPU (Mac/Intel/AMD) ã®å ´åˆ]
# --features cuda ã‚’å¤–ã™ã¨CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ï¼ˆé…ã„ã§ã™ãŒå‹•ä½œã—ã¾ã™ï¼‰
cargo run --release --bin train_llama

# 3. Inference (æŽ¨è«–)
# å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ä¼šè©±ã—ã¾ã™
cargo run --release --bin inference_llama
```

## ðŸ”Œ Python Integration (å¤–éƒ¨é€£æº)
Bit-TTT Engine ã¯ Python ã‹ã‚‰ DLL (`.so`/`.dylib`) ã¨ã—ã¦ç›´æŽ¥å‘¼ã³å‡ºã—å¯èƒ½ã§ã™ã€‚
(è©³ç´°ã¯ Rootã®READMEã‚’å‚ç…§ã—ã¦ãã ã•ã„)

---

## ðŸ’Ž Pre-trained Models (é…å¸ƒè¨ˆç”»)

ç¾åœ¨ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨å…¬é–‹ã‚’è¨ˆç”»ã—ã¦ã„ã¾ã™ã€‚

| Model Name | Specs | Training Data | Status | Download |
|---|---|---|---|---|
| **Bit-Llama-Micro** | ~11M Params, 1.58bit | TinyStories (Mini) | ðŸŸ¡ **Training** | *Coming Soon* |
| **Bit-Llama-Code** | ~100M Params, 1.58bit | Python Code Snippets | âšª Planned | - |

> **Note**: å­¦ç¿’æ¸ˆã¿é‡ã¿ (`.safetensors`) ã¯ Hugging Face Hub ã§ã®å…¬é–‹ã‚’äºˆå®šã—ã¦ã„ã¾ã™ã€‚

---

## ðŸ“Š ç¾çŠ¶ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ (Metrics)
*   **Training Speed**: ~800 tokens/sec (RTX 4060 Ti)
*   **Loss Curve**: Smooth convergence observed at Step 150 (Loss: 4.15).
*   **Generation**: "Always" -> "Alice" -> "Alice was very tired..." (Context learning observed).

## ðŸš€ Future Roadmap
1.  **Distributed Training**: Implement Data Parallelism for multi-GPU training.
2.  **Hugging Face Integration**: Provide `from_pretrained("bit-ttt/llama-11m")` API.
3.  **Desktop App**: Integrate into "Alice" desktop assistant (Phase 13 Step 5).
